{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning on Hamster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._trlib import TRLIBQuadraticSubproblem\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._group_columns import group_dense, group_sparse\n",
      "/Users/admin/miniconda2/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n",
      "slippery!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import sys\n",
    "env = gym.make('HamsterExperiment-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "#             print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes),)\n",
    "            sys.stdout.flush()   \n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        \n",
    "        # Set epsilon\n",
    "        epsilon = 1./(1.+i_episode)\n",
    "        # Observe S_0, t<-0\n",
    "        state = env.reset()\n",
    "        # Get A_0 from Q (epsilon-greedy policy) for this state\n",
    "        policy_state = epsilon_greedy_policy(env, Q[state], epsilon)\n",
    "        action = np.random.choice(np.arange(env.nA), p=policy_state)\n",
    "        \n",
    "        # Repeat until terminal state reached\n",
    "        while True:\n",
    "            # Take A_t, get R_(t+1), S_(t+1)\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Choose A_(t+1) from Q (from policy for S_(t+1))\n",
    "            policy_state = epsilon_greedy_policy(env, Q[state_next], epsilon)\n",
    "            action_next = np.random.choice(np.arange(env.nA), p=policy_state)\n",
    "            # Get G_t\n",
    "            G_t = reward + gamma*Q[state_next][action_next]\n",
    "            # Update action value function\n",
    "            Q[state][action] = Q[state][action] + alpha*(G_t - Q[state][action])\n",
    "#             print (Q[state][action])\n",
    "            \n",
    "            # print\n",
    "            if i_episode > 5000:\n",
    "                import os\n",
    "                os.system('cls')\n",
    "                from IPython.display import clear_output\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            # Check if reached terminal state\n",
    "            if done:\n",
    "                break\n",
    "            # Update state & action for next step\n",
    "            state = state_next\n",
    "            action = action_next\n",
    "        \n",
    "    return Q\n",
    "\n",
    "def epsilon_greedy_policy(env, Q_state, epsilon):\n",
    "    # Get greedy action (gives highest Q for state)\n",
    "    greedy_action = np.argmax(Q_state)\n",
    "    # Get number of possible actions     \n",
    "    nA = env.nA\n",
    "    # Use epsilon to get probability distribution to use in policy for state\n",
    "    policy_state = np.ones(nA) * epsilon / nA\n",
    "    policy_state[greedy_action] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_state\n",
    "\n",
    "def greedy_policy(env, Q_state):\n",
    "    # Get greedy action (gives highest Q for state)\n",
    "    greedy_action = np.argmax(Q_state)\n",
    "    # Get number of possible actions     \n",
    "    nA = env.nA\n",
    "    policy_state = np.zeros(nA)\n",
    "    policy_state[greedy_action] = 1\n",
    "    return policy_state\n",
    "\n",
    "def final_policy(env, Q):\n",
    "    # Observe S_0, t<-0\n",
    "    coords = [env.get_start()]\n",
    "    state = env.reset()\n",
    "    # Get A_0 from Q (epsilon-greedy policy) for this state\n",
    "    policy_state = greedy_policy(env, Q[state])\n",
    "    action = np.random.choice(np.arange(env.nA), p=policy_state)\n",
    "        \n",
    "    # Repeat until terminal state reached\n",
    "    while True:\n",
    "        # Take A_t, get R_(t+1), S_(t+1)\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "            \n",
    "        # Choose A_(t+1) from Q (from policy for S_(t+1))\n",
    "        policy_state = greedy_policy(env, Q[state_next])\n",
    "        action_next = np.random.choice(np.arange(env.nA), p=policy_state)\n",
    "            \n",
    "        coords.append(np.unravel_index(state_next, env.get_shape()))\n",
    "            \n",
    "        # Check if reached terminal state\n",
    "        if done:\n",
    "            break\n",
    "        # Update state & action for next step\n",
    "        state = state_next\n",
    "        action = action_next\n",
    "        \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.1\n",
      "□  □  □  □  □  □  □  □  □  □  □  □\n",
      "□  □  □  □  □  □  □  □  □  □  □  □\n",
      "□  □  □  □  □  □  □  □  □  □  □  □\n",
      "□  □  □  □  □  □  □  □  □  □  □  □\n",
      "□  □  □  □  □  □  □  □  □  □  □  □\n",
      "□  ×  ×  ×  ×  ×  ×  ×  ×  ×  ×  ★\n",
      "□  □  □  □  □  □  □  □  □  □  □  □\n",
      "\n",
      "\n",
      "Estimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\n",
      "[[ 1  3  2  1  1  1  0  0  1  1  3  3]\n",
      " [ 3  0  1  2  1  2  2  2  0  0  2  2]\n",
      " [ 1  2  2  1  0  2  2  2  2  2  2  2]\n",
      " [ 1  1  1  1  1  3  1  2  2  2  2  0]\n",
      " [ 1  0  0  0  3  0  3  1  3  1  1  2]\n",
      " [ 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0]\n",
      " [ 0  1  1  1  1  1 -1 -1 -1 -1 -1 -1]]\n",
      "[(6, 0), (5, 0), (4, 0), (4, 1), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (5, 11)]\n"
     ]
    }
   ],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa = sarsa(env, 5010, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(7*12)]).reshape(7,12)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)\n",
    "\n",
    "\n",
    "# transform quality map into actions\n",
    "# start = env.get_start()\n",
    "# for col in policy_sarsa:\n",
    "#     for row in col:\n",
    "#         if (col,row) == start:\n",
    "list_of_coords = final_policy(env, Q_sarsa)\n",
    "print(list_of_coords)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named PyObjCTools",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/admin/Documents/Hamster/lab6/run.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# noinspection PyUnresolvedReferences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mHamsterAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_ble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobotComm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTkinter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kcchang/Desktop/CS123/Hamster_Stanford/HamsterAPI/comm_ble.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named PyObjCTools"
     ]
    }
   ],
   "source": [
    "%run -i '/Users/admin/Documents/Hamster/lab6/run.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
